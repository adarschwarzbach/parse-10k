{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4af5ad9",
   "metadata": {},
   "source": [
    "This is my scrapbook. I wrote code here to work through my thought process, try and find patterns in the HTML for item 1, try bs4, regex, sec-api and more. It also contains the start of a function that could be used in a script to query the 10-k.htm links to validate my proof of concept with further efforts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b798d908",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sec_api import ExtractorApi\n",
    "from sec_api import QueryApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of a script to query the htm links for a companies 10-k \n",
    "# written with Lucene query language\n",
    "# currently returns a json table of values, one of which is the HTM link\n",
    "# Currently returns 10-k and sometimes extra 10-k/a values\n",
    "\n",
    "queryApi = QueryApi(\"b33c5215f9268b5f8356340e5452cff0e52d23d9a52d01e5a956409948f60bb6\")\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"formType:\\\"10-K\\\" AND ticker:(V)\"\n",
    "        }\n",
    "    },\n",
    "    \"from\": \"0\",\n",
    "    \"size\": \"30\",\n",
    "    \"sort\": [{ \"filedAt\": { \"order\": \"desc\" } }]\n",
    "}\n",
    "\n",
    "filings = queryApi.get_filings(query)\n",
    "print(filings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48dec9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function to download the 10-k txt and HTML file locally for a given company\n",
    "# creates folder titled \"given_ticker\" with subfolders for each year\n",
    "# folders found inside \"sec-edgar-fillings\"\n",
    "\n",
    "def getFile(company,):\n",
    "    dl = Downloader()\n",
    "    dl.get(\"10-K\", company)\n",
    "getFile(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8155ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download only the 10-k txt file locally for a given company\n",
    "# creates folder titled \"given_ticker\" with subfolders for each year\n",
    "# folders found inside \"sec-edgar-fillings\"\n",
    "def getFileTXT(company):\n",
    "    dl = Downloader()\n",
    "    dl.get(\"10-K\", company, download_details = False)\n",
    "getFileTXT(\"V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bs4 to try and search for item 1 within the 10-k\n",
    "# item 1 has minimal tags so hard to find outside of the 10-k index\n",
    "soup = BeautifulSoup(open(\"test.html\", encoding=\"utf8\"), \"html.parser\")\n",
    "\n",
    "print(soup.find_all( \"a\", string = \"Item 1. \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0bfcfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using bs4 with a lxml parser to look for patters in the 10-k html\n",
    "# using search arguement \"body\" to try and narrow scope\n",
    "\n",
    "soup = BeautifulSoup(open(\"test.html\", encoding=\"utf8\"), \"lxml\")\n",
    "x = soup.find(\"body\")\n",
    "print(x.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bs4 with a html parser to look for patters in the 10-k html\n",
    "\n",
    "soup = BeautifulSoup(open(\"test.html\", encoding=\"utf8\"), \"html.parser\")\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2f490",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# using bs4 and re to try and search, method notworking sucssesful\n",
    "\n",
    "f = open(\"test.txt\", \"r\")\n",
    "source = f.read()\n",
    "f.close()\n",
    "soup = BeautifulSoup(source, \"lxml\")\n",
    "print(soup.find_all(string = re.compile(\"part 1.\") ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66a5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of a brute force method to truncate the txt file \n",
    "# I stopped as this would not work - it would truncate necesary HTML\n",
    "# this would make the HTML object incomplete\n",
    "\n",
    "f = open(\"test.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "w = open(\"test_write.txt\",\"w\")\n",
    "countp1 = 0\n",
    "countpA = 0\n",
    "sp1 = \"item 1.\"\n",
    "spA = \"item 1a\"\n",
    "for line in lines:\n",
    "    if(sp1 in line):\n",
    "        sp1+=1\n",
    "    if(countp1 == 2 and countpA ==1):\n",
    "        w.write(line)\n",
    "    if(spA in line):\n",
    "        countpA+=1\n",
    "    if(countp1 == 2 and countpA ==2):\n",
    "        break\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inital attempts to use sec-api to parse the 10-k\n",
    "# url must be the sec.gov URL, not a local website\n",
    "\n",
    "extractorApi = ExtractorApi(\"b33c5215f9268b5f8356340e5452cff0e52d23d9a52d01e5a956409948f60bb6\")\n",
    "url = \"https://www.sec.gov/Archives/edgar/data/320193/000119312511282113/d220209d10k.htm\"\n",
    "secH = extractorApi.get_section(url, \"1\", \"html\")\n",
    "print(secH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ad0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used this cell to debug my function to write the json file for a dictionary\n",
    "\n",
    "dictionary = createDictionary(\"TSLA.txt\")\n",
    "print(dictionary)\n",
    "writeJSON(dictionary,\"TSLA\")\n",
    "#f = open(\"TSLA.json\", \"w\")\n",
    "#json.dump(dictionary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
